{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d12b446-d2fd-4361-85a9-59c953fcae24",
   "metadata": {},
   "source": [
    "# Llama\n",
    "\n",
    "```{note}\n",
    "Llama shows that it is possible to train state-of-the-art models using publicly available datasets exclusively.\n",
    "```\n",
    "\n",
    "paper: https://arxiv.org/abs/2302.13971\n",
    "\n",
    "github: https://github.com/facebookresearch/llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d63d06-5df8-49ba-b65c-a19b8942ad1d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "More parameters leads to better performance? Recent work shows that, for a given compute budget, the best performances are not achieved by the largest models, but smaller models trained on more data. Furthermore, smaller models are cheaper when inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95038e-9710-4d0b-a262-676fd5971a16",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "### Pre-training data\n",
    "\n",
    "| Dataset       | Sampling prop | Epochs | Type     |\n",
    "|---------------|---------------|--------|----------|\n",
    "| CommonCrawl   | 67.0%         | 1.10   | Websites |\n",
    "| C4            | 15.0%         | 1.16   | Websites |\n",
    "| Github        | 4.5%          | 0.64   | Code     |\n",
    "| Wikipedia     | 4.5%          | 2.45   | Multi languages  Wiki |\n",
    "| Books         | 4.5%          | 2.23   | Books    |\n",
    "| ArXiv         | 2.5%          | 1.06   | Science  |\n",
    "| StackExchange | 2.0%          | 1.03   | QA       |\n",
    "\n",
    "We tokenize the data with **byte pair encoding (BPE)** algorithm, the entire training dataset contains roughly 1.4T tokens after tokenization.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Our network is based on the transformer architecture, here are the main difference with the original architecture:\n",
    "\n",
    "* Pre-normalization [GPT3] To improve the training stability, from $\\text{Norm}(x + \\text{Sublayer}(x))$ to $\\text{Sublayer}(\\text{Norm}(x)) + x$, we use the **RMSNorm**\n",
    "\n",
    "* **SwiGLU** activation function [PaLM] replace ReLU by SwiGLU\n",
    "\n",
    "* **Rotary Embeddings** [GPTNeo] replace absolute positional embeddings with rotary positional embeddings (RoPE)\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "Trained using AdamW with $\\beta_{1} = 0.9$ and $\\beta_{2}=0.95$, we use a cosine learning rate schedule such that the final learning rate equals 10% of the maximal learning rate. 2000 warmup steps.\n",
    "\n",
    "We use a weight decay of 0.1 and gradient clipping  of 1.0\n",
    "\n",
    "### Efficient implementation\n",
    "\n",
    "* Use a efficient implementation of the casual(随机) multi-head attention to reduce memory usage, available in the **xformer** library.\n",
    "\n",
    "* Save the activations that are expensive to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358656b-7ad9-48c8-b584-fdf5b77f1007",
   "metadata": {},
   "source": [
    "## Main results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a18d1-fc05-4c92-884c-55e978609a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
