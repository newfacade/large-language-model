{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b5dbbf-a572-4ffc-859c-3c0f5e511e53",
   "metadata": {},
   "source": [
    "# Llama 2\n",
    "\n",
    "```{note}\n",
    "Llama 2 is a collection of pretrained and fine-tuned LLMs ranging from 7b to 70b parameters. The fine-tuned LLMs, called Llama2-chat, are optimized for dialogue use cases.\n",
    "```\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\n",
    "methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human\n",
    "Feedback (RLHF).\n",
    "\n",
    "We are releasing the following models to the general public for research and commercial use:\n",
    "\n",
    "* **Llama 2,** an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention.\n",
    "\n",
    "* **Llama 2-Chat,** a fine-tuned version of Llama 2 that is optimized for dialogue use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e5b92e-28bb-4a7d-953a-90918cda891d",
   "metadata": {},
   "source": [
    "## Pretraining\n",
    "\n",
    "### Pretraining Data\n",
    "\n",
    "We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off.\n",
    "\n",
    "### Training Details\n",
    "\n",
    "We adopt most of the pretraining setting and model architecture from Llama 1. We use the standard\n",
    "transformer architecture, apply pre-normalization using RMSNorm, use the SwiGLU activation function, and rotary positional embeddings. The primary architectural differences from Llama 1 include increased context length and **grouped-query attention (GQA)**.\n",
    "\n",
    "### Llama 2 Pretrained Model Evaluation\n",
    "\n",
    "We evaluate pretrained models on standard academic benchmarks, the benchmarks are grouped into the categories listed below:\n",
    "\n",
    "* Code: We report the average pass@1 scores of our models on HumanEval and MBPP.\n",
    "* Commensense reasoning.\n",
    "* World knowledge.\n",
    "* Reading comprehension.\n",
    "* Math: We report the average of the GSM8K(8 shot) and MATH (4 shot) benchmarks at top 1.\n",
    "* Popular aggregated benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b4e684-44a0-426d-8241-8a4d36f74b9b",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources.\n",
    "\n",
    "### Supervised Fine-Tuning (SFT)\n",
    "\n",
    "**Getting Started.** To bootstrap, we started the SFT stage with publicly available instruction tuning\n",
    "data.\n",
    "\n",
    "**Quality Is All You Need.** A limited set of clean instruction-tuning\n",
    "data can be sufficient to reach a high level of quality. We found that SFT annotations in the order of tens of thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of 27,540 annotations.\n",
    "\n",
    "We also observed that different annotation platforms and vendors can result in markedly different downstream\n",
    "model performance, highlighting the importance of data checks even when using vendors to source\n",
    "annotations. To validate our data quality, we carefully examined a set of 180 examples, comparing the annotations\n",
    "provided by humans with the samples generated by the model through manual scrutiny. Surprisingly,\n",
    "we found that the outputs sampled from the resulting SFT model were often competitive with SFT data\n",
    "handwritten by human annotators, suggesting that we could reprioritize and devote more annotation effort\n",
    "to preference-based annotation for RLHF.\n",
    "\n",
    "**Fine-Tuning Details.** For supervised fine-tuning, we use a cosine learning rate schedule with an initial learning rate of 2 × 10−5, a weight decay of 0.1, a batch size of 64, and a sequence length of 4096 tokens.\n",
    "\n",
    "For the fine-tuning process, each sample consists of a prompt and an answer. To ensure the model sequence\n",
    "length is properly filled, we concatenate all the prompts and answers from the training set. A special token is\n",
    "utilized to separate the prompt and answer segments. We utilize an autoregressive objective and zero-out\n",
    "the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens. Finally, we\n",
    "fine-tune the model for 2 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e119b8d-c169-4a6a-a5e9-543d23ad6269",
   "metadata": {},
   "source": [
    "### Reinforcement Learning with Human Feedback (RLHF)\n",
    "\n",
    "RLHF is a model training procedure that is applied to a fined-tuned language model to further align model behavior with human preferences and instructions following. We collect data that represents empirically sampled human preferences, whereby human annotators select which of two model outputs they prefer. This human feedback is subsequently used to train a reward model, which learns patterns in the preferences of the human annotators and can then automate preference decisions.\n",
    "\n",
    "#### Human Preference Data Collection\n",
    "\n",
    "We ask annotators to first write a prompt, then choose\n",
    "between two sampled model responses, based on provided criteria. we also ask annotators to label the degree to which they prefer their chosen response over the alternative: either their choice is *significantly better, better, slightly better, or negligibly better/ unsure*.\n",
    "\n",
    "For our collection of preference annotations, we focus on helpfulness and safety.\n",
    "\n",
    "Human annotations were collected in batches on a weekly basis. As we collected more preference data, our\n",
    "reward models improved, and we were able to train progressively better versions for Llama 2-Chat. It is important before a new Llama 2-Chat tuning iteration to\n",
    "gather new preference data using the latest Llama 2-Chat iterations. This step helps keep the reward model\n",
    "on-distribution and maintain an accurate reward for the latest model.\n",
    "\n",
    "#### Reward Modeling\n",
    "\n",
    "The reward model takes a model response and its corresponding prompt (including contexts from previous\n",
    "turns) as inputs and outputs a scalar score to indicate the quality of the model\n",
    "generation. Leveraging such response scores as rewards, we can optimize Llama 2-Chat during RLHF for\n",
    "better human preference alignment and improved helpfulness and safety.\n",
    "\n",
    "We train two separate reward\n",
    "models, one optimized for helpfulness (referred to as Helpfulness RM) and another for safety.\n",
    "\n",
    "We initialize our reward models from pretrained chat model checkpoints, as it ensures that both models\n",
    "benefit from knowledge acquired in pretraining. The model architecture and hyper-parameters are identical to those of the pretrained language models, except that the classification head for next-token prediction is replaced with a regression head for outputting a scalar reward.\n",
    "\n",
    "**Training Objectives.**  To train the reward model, we use the collected pairwise human preference data and a binary ranking loss:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{ranking}} = -\\log(\\sigma(r_{\\theta}(x, y_{c}) - r_{\\theta}(x, y_{r})))$$\n",
    "\n",
    "where $r_{\\theta}(x, y)$ is a scalar score output for prompt $x$ and completion $y$ with model weight $\\theta$. $y_{c}$ is the preferred response and $y_{r}$ is the rejected counterpart.\n",
    "\n",
    "Built on top of this binary ranking loss, given that our preference ratings is decomposed as a scale of four points (e.g., significantly better), it can be useful to leverage this information to explicitly\n",
    "teach the reward model to assign more discrepant scores to the generations that have more differences. To\n",
    "do so, we further add a margin component in the loss:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{ranking}} = -\\log(\\sigma(r_{\\theta}(x, y_{c}) - r_{\\theta}(x, y_{r}) - m(r)))$$\n",
    "\n",
    "where the margin $m(r)$ is a discrete function of the preference rating.\n",
    "\n",
    "**Data Composition.** We combine our newly collected data with existing open-source preference datasets\n",
    "to form a larger training dataset. Initially, open-source datasets were used to bootstrap our reward models while we were in the process of collecting preference annotation data.\n",
    "\n",
    "**Training Details.** We train for one epoch over the training data.\n",
    "\n",
    "**Reward Model Results.** As expected, our own reward models perform the best\n",
    "on our internal test sets collected based on Llama 2-Chat, with the Helpfulness reward model performing\n",
    "best on the Meta Helpfulness test set, and similarly the Safety reward model performing best on the Meta\n",
    "Safety test set.\n",
    "\n",
    "**Scaling Trends.** As expected, More data and a larger-size model generally improve\n",
    "accuracy. More importantly, the scaling performance\n",
    "has not yet plateaued given the existing volume of data annotation used for training, a signal that there is\n",
    "room for more improvement with more annotations. We note that reward model accuracy is one of the most\n",
    "important proxies for the final performance of Llama 2-Chat. While best practices for comprehensively\n",
    "evaluating a generative model is an open research question, the ranking task of the reward has no ambiguity.\n",
    "Therefore, everything else being equal, an improvement of the reward model can be directly translated into\n",
    "an improvement for Llama 2-Chat.\n",
    "\n",
    "#### Iterative Fine-Tuning\n",
    "\n",
    "As we received more batches of human preference data annotation, we were able to train better reward\n",
    "models and collect more prompts. We therefore trained successive versions for RLHF models, referred to\n",
    "here as RLHF-V1, . . . , RLHF-V5.\n",
    "\n",
    "We explored RLHF fine-tuning with two main algorithms:\n",
    "\n",
    "**Rejection Sampling fine-tuning.** At each iterative stage, we sample K answers for each prompt from the most recent model. We score each\n",
    "sample given the best reward model accessible at the time of the experiment, and then select the best answer for a given prompt. \n",
    "\n",
    "In earlier versions of our model, up to RLHF V3, our approach was to confine answer\n",
    "selection solely to the “bag” of samples gathered from the preceding iteration. For example, RLHF V3 was\n",
    "trained using only samples from RLHF V2. However, this method led to a regression in some capabilities. In response, on subsequent iterations, we modified our strategy, incorporating top-performing samples from\n",
    "all prior iterations, such as those used in RLHF-V1 and RLHF-V2.\n",
    "\n",
    "We illustrate the benefit of Rejection Sampling in Figure 7. The delta between the maximum and median\n",
    "curves can be interpreted as the potential gain of fine-tuning on the best output.\n",
    "\n",
    "![](../images/llama2-rejective.png)\n",
    "\n",
    "We can observe that\n",
    "the optimal temperature is not constant during the iterative model updates: RLHF has a direct impact on\n",
    "rescaling the temperature.\n",
    "\n",
    "We perform rejection sampling only with our largest 70B Llama 2-Chat. All smaller\n",
    "models are fine-tuned on rejection sampled data from the larger model, thus distilling the large-model\n",
    "capabilities into the smaller ones.\n",
    "\n",
    "**PPO.** We further train our language model following the RL scheme which uses the\n",
    "reward model as an estimate for the true reward function (human preference) and the pretrained language\n",
    "model as the policy to optimize. During this phase, we seek to optimize the following objective:\n",
    "\n",
    "$$\\arg\\underset{\\pi}{\\max} \\mathbb{E}_{p\\sim\\mathcal{D},g\\sim\\pi}[R(g|p)]$$\n",
    "\n",
    "We iteratively improve the policy by sampling prompts $p$ from our dataset $\\mathcal{D}$ and generations $g$ from the\n",
    "policy $\\pi$ and use the PPO algorithm and loss function to achieve this objective.\n",
    "\n",
    "The final reward function we use during optimization:\n",
    "\n",
    "$$R(g|p) = \\tilde{R}_{c}(g|p) - \\beta D_{KL}(\\pi_{\\theta}(g|p) \\| \\pi_{0}(g|p))$$\n",
    "\n",
    "contains a penalty term for diverging from the original policy $\\pi_{0}$. We find this constraint is useful for training stability, and to reduce reward\n",
    "hacking wherebywewould achieve high scores from the reward model but lowscores from human evaluation.\n",
    "\n",
    "We define $R_{c}$ to be a piecewise combination of the safety $(R_{s})$ and helpfulness $(R_h)$ reward models. We also find it important to whiten\n",
    "the final linear scores (shown here by reversing the sigmoid with the logit function):\n",
    "\n",
    "$$\n",
    "R_{c}(g|p) = \n",
    "\\begin{cases}\n",
    "R_{s}(g|p)\\quad &\\text{if IS_SAFETY($p$) or }R_{s}(g|p)<0.15\\\\\n",
    "R_{h}(g|p)\\quad &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{R}_{c}(g|p) = \\text{WHITEN}(\\text{LOGIT}(R_{c}(g|p)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ee5a8-a531-4999-b3e9-ea5057864332",
   "metadata": {},
   "source": [
    "### System Message for Multi-Turn Consistency\n",
    "\n",
    "In a dialogue setup, some instructions should apply for all the conversation turns, e.g., to respond succinctly,\n",
    "or to “act as” some public figure. When we provided such instructions to Llama 2-Chat, the subsequent\n",
    "response should always respect the constraint. However, our initial RLHF models tended to forget the initial\n",
    "instruction after a few turns of dialogue. To address these limitations, we propose Ghost Attention (GAtt).\n",
    "\n",
    "**GAtt Method.** Assume we have access to a multi-turn dialogue dataset between two persons (e.g., a user\n",
    "and an assistant), with a list of messages $[u_1, a_1, u_2, a_2, \\dots , u_n, a_n]$, where $u_{n}$ and $a_{n}$ correspond to the user and\n",
    "assistant messages for turn $n$, respectively. Then, we define an instruction, $inst$, that should be respected\n",
    "throughout the dialogue. For example, $inst$ could be “act as.”\n",
    "\n",
    "Next, we can sample from this synthetic data $[inst + u_{1},a_{1}, inst + u_{2},a_{2},\\dots, inst + u_{n}, a_{n}]$ using the latest RLHF model. We now have a context-dialogue\n",
    "and the sample with which to fine-tune a model. Instead of\n",
    "augmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, generate $[inst + u_{1},a_{1}, u_{2},a_{2},\\dots, u_{n}, a_{n}]$ but this\n",
    "would lead to a mismatch at training time between the system message. To fix this issue, which could hurt the training, we\n",
    "simply set the loss to 0 for all the tokens from the previous turns, including assistant messages.\n",
    "\n",
    "We applied GAtt after RLHF V3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29698d02-7ded-4307-8d16-90d838106b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}