

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Llama 2 &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'base-llms/llama2';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../llama.html">Llama</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llama2.html">Llama 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../instruct-gpt.html">Instruct GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cruxeval.html">CRUXEval</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dpo.html">DPO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rso.html">RSO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dpop.html">DPOP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../magic.html">Magicoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wizardlm.html">WizardLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lora.html">LORA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../alphacode.html">AlphaCode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../alphacode2.html">AlphaCode 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gold.html">GOLD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../alphacodium.html">AlphaCodium</a></li>
<li class="toctree-l1"><a class="reference internal" href="../weak-to-strong.html">Weak to Strong Generalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../scaling-law.html">Scaling Laws for Neural Language Models</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fbase-llms/llama2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/base-llms/llama2.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Llama 2</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining">Pretraining</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining-data">Pretraining Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-details">Training Details</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-2-pretrained-model-evaluation">Llama 2 Pretrained Model Evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">Fine-Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-fine-tuning-sft">Supervised Fine-Tuning (SFT)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-with-human-feedback-rlhf">Reinforcement Learning with Human Feedback (RLHF)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#human-preference-data-collection">Human Preference Data Collection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-modeling">Reward Modeling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-fine-tuning">Iterative Fine-Tuning</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#system-message-for-multi-turn-consistency">System Message for Multi-Turn Consistency</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llama-2">
<h1>Llama 2<a class="headerlink" href="#llama-2" title="Permalink to this heading">#</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Llama 2 is a collection of pretrained and fine-tuned LLMs ranging from 7b to 70b parameters. The fine-tuned LLMs, called Llama2-chat, are optimized for dialogue use cases.</p>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training
methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human
Feedback (RLHF).</p>
<p>We are releasing the following models to the general public for research and commercial use:</p>
<ul class="simple">
<li><p><strong>Llama 2,</strong> an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention.</p></li>
<li><p><strong>Llama 2-Chat,</strong> a fine-tuned version of Llama 2 that is optimized for dialogue use cases.</p></li>
</ul>
</section>
<section id="pretraining">
<h2>Pretraining<a class="headerlink" href="#pretraining" title="Permalink to this heading">#</a></h2>
<section id="pretraining-data">
<h3>Pretraining Data<a class="headerlink" href="#pretraining-data" title="Permalink to this heading">#</a></h3>
<p>We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off.</p>
</section>
<section id="training-details">
<h3>Training Details<a class="headerlink" href="#training-details" title="Permalink to this heading">#</a></h3>
<p>We adopt most of the pretraining setting and model architecture from Llama 1. We use the standard
transformer architecture, apply pre-normalization using RMSNorm, use the SwiGLU activation function, and rotary positional embeddings. The primary architectural differences from Llama 1 include increased context length and <strong>grouped-query attention (GQA)</strong>.</p>
</section>
<section id="llama-2-pretrained-model-evaluation">
<h3>Llama 2 Pretrained Model Evaluation<a class="headerlink" href="#llama-2-pretrained-model-evaluation" title="Permalink to this heading">#</a></h3>
<p>We evaluate pretrained models on standard academic benchmarks, the benchmarks are grouped into the categories listed below:</p>
<ul class="simple">
<li><p>Code: We report the average pass&#64;1 scores of our models on HumanEval and MBPP.</p></li>
<li><p>Commensense reasoning.</p></li>
<li><p>World knowledge.</p></li>
<li><p>Reading comprehension.</p></li>
<li><p>Math: We report the average of the GSM8K(8 shot) and MATH (4 shot) benchmarks at top 1.</p></li>
<li><p>Popular aggregated benchmarks.</p></li>
</ul>
</section>
</section>
<section id="fine-tuning">
<h2>Fine-Tuning<a class="headerlink" href="#fine-tuning" title="Permalink to this heading">#</a></h2>
<p>Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources.</p>
<section id="supervised-fine-tuning-sft">
<h3>Supervised Fine-Tuning (SFT)<a class="headerlink" href="#supervised-fine-tuning-sft" title="Permalink to this heading">#</a></h3>
<p><strong>Getting Started.</strong> To bootstrap, we started the SFT stage with publicly available instruction tuning
data.</p>
<p><strong>Quality Is All You Need.</strong> A limited set of clean instruction-tuning
data can be sufficient to reach a high level of quality. We found that SFT annotations in the order of tens of thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of 27,540 annotations.</p>
<p>We also observed that different annotation platforms and vendors can result in markedly different downstream
model performance, highlighting the importance of data checks even when using vendors to source
annotations. To validate our data quality, we carefully examined a set of 180 examples, comparing the annotations
provided by humans with the samples generated by the model through manual scrutiny. Surprisingly,
we found that the outputs sampled from the resulting SFT model were often competitive with SFT data
handwritten by human annotators, suggesting that we could reprioritize and devote more annotation effort
to preference-based annotation for RLHF.</p>
<p><strong>Fine-Tuning Details.</strong> For supervised fine-tuning, we use a cosine learning rate schedule with an initial learning rate of 2 × 10−5, a weight decay of 0.1, a batch size of 64, and a sequence length of 4096 tokens.</p>
<p>For the fine-tuning process, each sample consists of a prompt and an answer. To ensure the model sequence
length is properly filled, we concatenate all the prompts and answers from the training set. A special token is
utilized to separate the prompt and answer segments. We utilize an autoregressive objective and zero-out
the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens. Finally, we
fine-tune the model for 2 epochs.</p>
</section>
<section id="reinforcement-learning-with-human-feedback-rlhf">
<h3>Reinforcement Learning with Human Feedback (RLHF)<a class="headerlink" href="#reinforcement-learning-with-human-feedback-rlhf" title="Permalink to this heading">#</a></h3>
<p>RLHF is a model training procedure that is applied to a fined-tuned language model to further align model behavior with human preferences and instructions following. We collect data that represents empirically sampled human preferences, whereby human annotators select which of two model outputs they prefer. This human feedback is subsequently used to train a reward model, which learns patterns in the preferences of the human annotators and can then automate preference decisions.</p>
<section id="human-preference-data-collection">
<h4>Human Preference Data Collection<a class="headerlink" href="#human-preference-data-collection" title="Permalink to this heading">#</a></h4>
<p>We ask annotators to first write a prompt, then choose
between two sampled model responses, based on provided criteria. we also ask annotators to label the degree to which they prefer their chosen response over the alternative: either their choice is <em>significantly better, better, slightly better, or negligibly better/ unsure</em>.</p>
<p>For our collection of preference annotations, we focus on helpfulness and safety.</p>
<p>Human annotations were collected in batches on a weekly basis. As we collected more preference data, our
reward models improved, and we were able to train progressively better versions for Llama 2-Chat. It is important before a new Llama 2-Chat tuning iteration to
gather new preference data using the latest Llama 2-Chat iterations. This step helps keep the reward model
on-distribution and maintain an accurate reward for the latest model.</p>
</section>
<section id="reward-modeling">
<h4>Reward Modeling<a class="headerlink" href="#reward-modeling" title="Permalink to this heading">#</a></h4>
<p>The reward model takes a model response and its corresponding prompt (including contexts from previous
turns) as inputs and outputs a scalar score to indicate the quality of the model
generation. Leveraging such response scores as rewards, we can optimize Llama 2-Chat during RLHF for
better human preference alignment and improved helpfulness and safety.</p>
<p>We train two separate reward
models, one optimized for helpfulness (referred to as Helpfulness RM) and another for safety.</p>
<p>We initialize our reward models from pretrained chat model checkpoints, as it ensures that both models
benefit from knowledge acquired in pretraining. The model architecture and hyper-parameters are identical to those of the pretrained language models, except that the classification head for next-token prediction is replaced with a regression head for outputting a scalar reward.</p>
<p><strong>Training Objectives.</strong>  To train the reward model, we use the collected pairwise human preference data and a binary ranking loss:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{ranking}} = -\log(\sigma(r_{\theta}(x, y_{c}) - r_{\theta}(x, y_{r})))\]</div>
<p>where <span class="math notranslate nohighlight">\(r_{\theta}(x, y)\)</span> is a scalar score output for prompt <span class="math notranslate nohighlight">\(x\)</span> and completion <span class="math notranslate nohighlight">\(y\)</span> with model weight <span class="math notranslate nohighlight">\(\theta\)</span>. <span class="math notranslate nohighlight">\(y_{c}\)</span> is the preferred response and <span class="math notranslate nohighlight">\(y_{r}\)</span> is the rejected counterpart.</p>
<p>Built on top of this binary ranking loss, given that our preference ratings is decomposed as a scale of four points (e.g., significantly better), it can be useful to leverage this information to explicitly
teach the reward model to assign more discrepant scores to the generations that have more differences. To
do so, we further add a margin component in the loss:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{ranking}} = -\log(\sigma(r_{\theta}(x, y_{c}) - r_{\theta}(x, y_{r}) - m(r)))\]</div>
<p>where the margin <span class="math notranslate nohighlight">\(m(r)\)</span> is a discrete function of the preference rating.</p>
<p><strong>Data Composition.</strong> We combine our newly collected data with existing open-source preference datasets
to form a larger training dataset. Initially, open-source datasets were used to bootstrap our reward models while we were in the process of collecting preference annotation data.</p>
<p><strong>Training Details.</strong> We train for one epoch over the training data.</p>
<p><strong>Reward Model Results.</strong> As expected, our own reward models perform the best
on our internal test sets collected based on Llama 2-Chat, with the Helpfulness reward model performing
best on the Meta Helpfulness test set, and similarly the Safety reward model performing best on the Meta
Safety test set.</p>
<p><strong>Scaling Trends.</strong> As expected, More data and a larger-size model generally improve
accuracy. More importantly, the scaling performance
has not yet plateaued given the existing volume of data annotation used for training, a signal that there is
room for more improvement with more annotations. We note that reward model accuracy is one of the most
important proxies for the final performance of Llama 2-Chat. While best practices for comprehensively
evaluating a generative model is an open research question, the ranking task of the reward has no ambiguity.
Therefore, everything else being equal, an improvement of the reward model can be directly translated into
an improvement for Llama 2-Chat.</p>
</section>
<section id="iterative-fine-tuning">
<h4>Iterative Fine-Tuning<a class="headerlink" href="#iterative-fine-tuning" title="Permalink to this heading">#</a></h4>
<p>As we received more batches of human preference data annotation, we were able to train better reward
models and collect more prompts. We therefore trained successive versions for RLHF models, referred to
here as RLHF-V1, … , RLHF-V5.</p>
<p>We explored RLHF fine-tuning with two main algorithms:</p>
<p><strong>Rejection Sampling fine-tuning.</strong> At each iterative stage, we sample K answers for each prompt from the most recent model. We score each
sample given the best reward model accessible at the time of the experiment, and then select the best answer for a given prompt.</p>
<p>In earlier versions of our model, up to RLHF V3, our approach was to confine answer
selection solely to the “bag” of samples gathered from the preceding iteration. For example, RLHF V3 was
trained using only samples from RLHF V2. However, this method led to a regression in some capabilities. In response, on subsequent iterations, we modified our strategy, incorporating top-performing samples from
all prior iterations, such as those used in RLHF-V1 and RLHF-V2.</p>
<p>We illustrate the benefit of Rejection Sampling in Figure 7. The delta between the maximum and median
curves can be interpreted as the potential gain of fine-tuning on the best output.</p>
<p><img alt="" src="../_images/llama2-rejective.png" /></p>
<p>We can observe that
the optimal temperature is not constant during the iterative model updates: RLHF has a direct impact on
rescaling the temperature.</p>
<p>We perform rejection sampling only with our largest 70B Llama 2-Chat. All smaller
models are fine-tuned on rejection sampled data from the larger model, thus distilling the large-model
capabilities into the smaller ones.</p>
<p><strong>PPO.</strong> We further train our language model following the RL scheme which uses the
reward model as an estimate for the true reward function (human preference) and the pretrained language
model as the policy to optimize. During this phase, we seek to optimize the following objective:</p>
<div class="math notranslate nohighlight">
\[\arg\underset{\pi}{\max} \mathbb{E}_{p\sim\mathcal{D},g\sim\pi}[R(g|p)]\]</div>
<p>We iteratively improve the policy by sampling prompts <span class="math notranslate nohighlight">\(p\)</span> from our dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> and generations <span class="math notranslate nohighlight">\(g\)</span> from the
policy <span class="math notranslate nohighlight">\(\pi\)</span> and use the PPO algorithm and loss function to achieve this objective.</p>
<p>The final reward function we use during optimization:</p>
<div class="math notranslate nohighlight">
\[R(g|p) = \tilde{R}_{c}(g|p) - \beta D_{KL}(\pi_{\theta}(g|p) \| \pi_{0}(g|p))\]</div>
<p>contains a penalty term for diverging from the original policy <span class="math notranslate nohighlight">\(\pi_{0}\)</span>. We find this constraint is useful for training stability, and to reduce reward
hacking wherebywewould achieve high scores from the reward model but lowscores from human evaluation.</p>
<p>We define <span class="math notranslate nohighlight">\(R_{c}\)</span> to be a piecewise combination of the safety <span class="math notranslate nohighlight">\((R_{s})\)</span> and helpfulness <span class="math notranslate nohighlight">\((R_h)\)</span> reward models. We also find it important to whiten
the final linear scores (shown here by reversing the sigmoid with the logit function):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
R_{c}(g|p) = 
\begin{cases}
R_{s}(g|p)\quad &amp;\text{if IS_SAFETY($p$) or }R_{s}(g|p)&lt;0.15\\
R_{h}(g|p)\quad &amp;\text{otherwise}
\end{cases}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
\tilde{R}_{c}(g|p) = \text{WHITEN}(\text{LOGIT}(R_{c}(g|p)))
\]</div>
</section>
</section>
<section id="system-message-for-multi-turn-consistency">
<h3>System Message for Multi-Turn Consistency<a class="headerlink" href="#system-message-for-multi-turn-consistency" title="Permalink to this heading">#</a></h3>
<p>In a dialogue setup, some instructions should apply for all the conversation turns, e.g., to respond succinctly,
or to “act as” some public figure. When we provided such instructions to Llama 2-Chat, the subsequent
response should always respect the constraint. However, our initial RLHF models tended to forget the initial
instruction after a few turns of dialogue. To address these limitations, we propose Ghost Attention (GAtt).</p>
<p><strong>GAtt Method.</strong> Assume we have access to a multi-turn dialogue dataset between two persons (e.g., a user
and an assistant), with a list of messages <span class="math notranslate nohighlight">\([u_1, a_1, u_2, a_2, \dots , u_n, a_n]\)</span>, where <span class="math notranslate nohighlight">\(u_{n}\)</span> and <span class="math notranslate nohighlight">\(a_{n}\)</span> correspond to the user and
assistant messages for turn <span class="math notranslate nohighlight">\(n\)</span>, respectively. Then, we define an instruction, <span class="math notranslate nohighlight">\(inst\)</span>, that should be respected
throughout the dialogue. For example, <span class="math notranslate nohighlight">\(inst\)</span> could be “act as.”</p>
<p>Next, we can sample from this synthetic data <span class="math notranslate nohighlight">\([inst + u_{1},a_{1}, inst + u_{2},a_{2},\dots, inst + u_{n}, a_{n}]\)</span> using the latest RLHF model. We now have a context-dialogue
and the sample with which to fine-tune a model. Instead of
augmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, generate <span class="math notranslate nohighlight">\([inst + u_{1},a_{1}, u_{2},a_{2},\dots, u_{n}, a_{n}]\)</span> but this
would lead to a mismatch at training time between the system message. To fix this issue, which could hurt the training, we
simply set the loss to 0 for all the tokens from the previous turns, including assistant messages.</p>
<p>We applied GAtt after RLHF V3.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./base-llms"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining">Pretraining</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining-data">Pretraining Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-details">Training Details</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-2-pretrained-model-evaluation">Llama 2 Pretrained Model Evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">Fine-Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-fine-tuning-sft">Supervised Fine-Tuning (SFT)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-with-human-feedback-rlhf">Reinforcement Learning with Human Feedback (RLHF)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#human-preference-data-collection">Human Preference Data Collection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reward-modeling">Reward Modeling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-fine-tuning">Iterative Fine-Tuning</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#system-message-for-multi-turn-consistency">System Message for Multi-Turn Consistency</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>