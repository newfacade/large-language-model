{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde9804c-72f6-4efd-a8be-64d34d536d29",
   "metadata": {},
   "source": [
    "# OpenRLHF\n",
    "\n",
    "```{note}\n",
    "An Easy-to-use, Scalable and\n",
    "High-performance RLHF Framework.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f549def-7791-4bb3-a2da-a3c04c8cbdee",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As large language models (LLMs) continue to grow by scaling laws, reinforcement\n",
    "learning from human feedback (RLHF) has gained significant attention due to its\n",
    "outstanding performance. However, as models grow larger, vanilla\n",
    "RLHF typically requires maintaining multiple models and a more complex learning pipeline, e.g. PPO requires maintaining four models\n",
    "during training, leading\n",
    "to increased demands for memory and computational resources.\n",
    "\n",
    "```{tip}\n",
    "The four models PPO requires are SFT model, reward model, actor and critic.\n",
    "```\n",
    "\n",
    "Existing open-source RLHF frameworks such as Transformer Reinforcement Learning (TRL), rely on parallelization approaches like Zero\n",
    "Redundancy Optimizer (ZeRO) to co-locate the four models involved in RLHF training on the same\n",
    "GPU. However, as models continue to grow past 70 billion parameters, this scheduling\n",
    "approach becomes increasingly inefficient with limited GPU memory. To address the limitations of\n",
    "co-location, some frameworks like TRL compromise on memory usage by `merging the actor and\n",
    "critic models` or employing techniques like Low-Rank Adaptation (LoRA). However, these\n",
    "can reduce model performance.\n",
    "\n",
    "```{tip}\n",
    "ZeRO (Zero Redundancy Optimizer) is a memory optimization technology for large-scale deep learning models. Developed by Microsoft as part of the DeepSpeed library, it aims to reduce memory usage and enhance training efficiency. ZeRO achieves this by partitioning model states, optimizer states, and gradients across multiple GPUs, allowing for the training of models with billions of parameters. It has three optimization stages: ZeRO-1 (optimizer state partitioning), ZeRO-2 (gradient partitioning), and ZeRO-3 (parameter partitioning), which progressively improve memory efficiency and scalability.\n",
    "```\n",
    "\n",
    "To enable easy RLHF training at scale, OpenRLHF `redesigns model scheduling using Ray, vLLM and DeepSpeed`, enabling training of models beyond 70 billion parameters.\n",
    "\n",
    "**PPO Support Matrix** \n",
    "\n",
    "| Feature | OpenRLHF | DSChat | CAIChat | TRL |\n",
    "| ------------- |:-------------:| :-------------:| :-------------:| :-------------:|\n",
    "| 70B+ Full Tuning with 16 A100-80GB      | ✅ | ❌ | ❌ | ❌ |\n",
    "| 7B Full Tuning with 4 RTX4090 | ✅      |    ❌ | ❌ | ❌ |\n",
    "| 34B DPO Full Tuning with 8 A100-80GB | ✅      |    ❌ | ❌ | ❌ |  \n",
    "| Inference Engine in PPO | ✅      |    ✅ | ❌ | ❌ |  \n",
    "| PPO Implementation Tricks | ✅      |    ❌ | ❌ | ✅ |\n",
    "| Support QLoRA | ✅      |    ❌ | ❌ | ✅ | \n",
    "| Support Mixtral 8*7b | ✅      |    ❌ | ❌ | ❌ |  \n",
    "| Support Unmerged Actor-Critic | ✅     |   ✅ | ✅ | ❌ | \n",
    "| Support Multiple Reward Models | ✅      |    ❌ | ❌ | ❌ |   \n",
    "| Support Huggingface Models | ✅      |    ✅ | ✅ | ✅ | \n",
    "| Easy-to-use | ✅      |   ❌ (HybridEngine bugs) | ✅ | ✅ | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95477c-d6ea-477a-bcb7-3df8080576f4",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Reinforcement Learning from Human Feedback\n",
    "\n",
    "The classic training of large language models based on a pre-trained Generative Pre-trained\n",
    "Transformer (GPT) involves three steps: Supervised Fine-tuning (SFT), Reward Model (RM) training,\n",
    "and PPO training.\n",
    "![](../images/trl1.png)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{PPO_objective} = &\\mathbb{E}_{(x, y)\\sim D_{\\pi_{\\phi}^{\\text{RL}}}}\\left[r(x, y) - \\beta\\log\\left(\\frac{\\pi_{\\phi}^{\\text{RL}}(y|x)}{\\pi^{\\text{SFT}}(y|x)}\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Ray\n",
    "\n",
    "```{note}\n",
    "Ray is a distributed execution framework that provides powerful scheduling and scaling capabilities\n",
    "for parallel and distributed computing workloads.\n",
    "```\n",
    "\n",
    "### vLLM\n",
    "\n",
    "```{note}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc85879-a83b-4fe1-89a7-9f76d9524536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
